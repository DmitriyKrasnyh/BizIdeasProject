# backend/gpt_bot.py
"""
Ğ¢ĞµĞ»ĞµĞ³Ñ€Ğ°Ğ¼-Ğ±Ğ¾Ñ‚ BizIdeas, Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰Ğ¸Ğ¹ÑÑ Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ LLM
(serve.py) Ğ¸ Ğ¿Ğ¸ÑˆÑƒÑ‰Ğ¸Ğ¹ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³ Ğ² Supabase.

ĞĞĞ’ĞĞ•:
â€¢ ĞŸÑ€Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ° Ğ¼Ğ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ĞµÑ‚ Â«â³ ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ÑƒĞ¼Ğ°ĞµÑ‚â€¦Â»
  Ğ¸ Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ‚Ğ¾Ğ³Ğ¾ Ğ¶Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ, Ğ¿Ğ¾ĞºĞ°
  Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ½Ğµ ÑÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½.
"""
import os, sys, subprocess, time, psutil, requests, textwrap
from pathlib  import Path
from uuid     import uuid4
from datetime import datetime, timezone
from typing   import Dict, Set

from aiogram import Bot, Dispatcher, types
from aiogram.types import InlineKeyboardMarkup, InlineKeyboardButton
from aiogram.utils import executor
from dotenv  import load_dotenv
from supabase import create_client, Client, AuthError

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ENV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()
TG_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
SB_URL   = os.getenv("VITE_SUPABASE_URL")
SB_KEY   = os.getenv("VITE_SUPABASE_ANON_KEY")

MODEL_PATH = Path(os.getenv("LLM_MODEL_PATH","backend/llm/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf")).resolve()
LLM_HOST   = os.getenv("LLM_HOST", "127.0.0.1")
LLM_PORT   = int(os.getenv("LLM_PORT", 8000))
SERVE_PY   = Path(__file__).parent / "llm" / "serve.py"
HEALTH_URL = f"http://{LLM_HOST}:{LLM_PORT}/health"
LLM_URL    = f"http://{LLM_HOST}:{LLM_PORT}/v1/completions"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ PROMPT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PROMPT_TMPL = """
Ğ¢Ñ‹ â€” Ğ¾Ğ¿Ñ‹Ñ‚Ğ½Ñ‹Ğ¹ Ğ±Ğ¸Ğ·Ğ½ĞµÑ-ĞºĞ¾Ğ½ÑÑƒĞ»ÑŒÑ‚Ğ°Ğ½Ñ‚.
ĞÑ‚Ğ²ĞµÑ‡Ğ°Ğ¹ Ğ»Ğ°ĞºĞ¾Ğ½Ğ¸Ñ‡Ğ½Ğ¾ Ğ¸ Ğ¿Ğ¾-Ğ´ĞµĞ»Ñƒ, Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼, Ñ†Ğ¸Ñ„Ñ€Ñ‹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ÑÑ.

â—¾ Ğ ĞµĞ³Ğ¸Ğ¾Ğ½ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ°: {region}
â—¾ ĞÑ‚Ñ€Ğ°ÑĞ»ÑŒ:        {sector}
â—¾ ĞĞ¿Ñ‹Ñ‚:           {exp}
â—¾ Ğ¦ĞµĞ»ÑŒ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ°:  {goal}

ğŸ’¡ Ğ’Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ
Â«{idea_title}Â» â€” {idea_descr}

â“ Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ
{question}

===== Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° (ÑĞ¾Ğ±Ğ»ÑĞ´Ğ°Ğ¹ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾) =====
1. ğŸ” ĞšĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¹ Â«Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾Ğ·Â» (1â€“2 Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ).
2. âœ… 3 Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ñ… ÑˆĞ°Ğ³Ğ° (âª… 40 ÑĞ»Ğ¾Ğ² ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹).
3. âš ï¸ 2 Ñ€Ğ¸ÑĞºĞ° + ĞºĞ°Ğº Ğ¸Ñ… ÑĞ½ÑÑ‚ÑŒ.
4. ğŸ“ˆ ĞŸĞ»Ğ°Ğ½ Ğ½Ğ° 3 Ğ¼ĞµÑ: Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ° â€œÑˆĞ°Ğ³ â†’ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°â€.
5. â­ Ğ’Ğ¾Ğ¿Ñ€Ğ¾Ñ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ñƒ Ğ¾Ğ´Ğ½Ğ¸Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ (CTA).
=========================================

### ĞÑ‚Ğ²ĞµÑ‚
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ helper-Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ supabase â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _sb_exec(req):
    """Wrapper, ĞºĞ¸Ğ´Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¸ÑĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞµ"""
    try:
        res = req.execute()
    except AuthError as e:
        raise RuntimeError(f"Supabase AuthError: {e}") from None

    data  = res.get("data")  if isinstance(res, dict) else getattr(res, "data", None)
    error = res.get("error") if isinstance(res, dict) else getattr(res, "error", None)
    if error:
        raise RuntimeError(error["message"])
    return data


def llm_running() -> bool:
    for p in psutil.process_iter(["cmdline"]):
        if "serve.py" in " ".join(p.info["cmdline"] or []) and str(LLM_PORT) in p.info["cmdline"]:
            return True
    return False


def start_llm() -> None:
    if llm_running():
        print("âœ…  LLM already running"); return

    if not SERVE_PY.exists():
        sys.exit("â›”  backend/llm/serve.py not found")

    cmd = [sys.executable, str(SERVE_PY), "--host", LLM_HOST, "--port", str(LLM_PORT), "--model", str(MODEL_PATH)]
    print("â–¶ï¸  Starting LLM:", " ".join(cmd))
    subprocess.Popen(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

    for _ in range(30):
        try:
            if requests.get(HEALTH_URL, timeout=1).ok:
                print("âœ…  LLM is up"); return
        except requests.RequestException:
            time.sleep(1)
    sys.exit("â›”  LLM didnâ€™t start in 30 sec")


def send_to_llm(prompt: str) -> str:
    r = requests.post(LLM_URL, json={"prompt": prompt, "max_tokens": 512, "temperature": 0.7}, timeout=180)
    r.raise_for_status()
    return r.json()["choices"][0]["text"].strip()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Supabase chat helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_or_create_session(sb: Client, user_id: int, idea_id: int) -> str:
    row = _sb_exec(sb.table("chat_sessions").select("session_id").eq("user_id", user_id).eq("idea_id", idea_id).limit(1).maybe_single())
    if row:
        return row["session_id"]
    sid = str(uuid4())
    _sb_exec(sb.table("chat_sessions").insert({"session_id": sid, "user_id": user_id, "idea_id": idea_id, "title": "Telegram chat"}))
    return sid


def log_chat(sb: Client, session_id: str, user_id: int, role: str, text: str):
    _sb_exec(sb.table("chat_messages").insert({
        "message_id": str(uuid4()),
        "session_id": session_id,
        "user_id": user_id,
        "role": role,
        "content": text,
        "created_at": datetime.now(timezone.utc).isoformat()
    }))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ğ˜ĞĞ˜Ğ¦Ğ˜ĞĞ›Ğ˜Ğ—ĞĞ¦Ğ˜Ğ¯ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if not all([TG_TOKEN, SB_URL, SB_KEY]):
    sys.exit("â›”  .env missing TG_TOKEN / SB_URL / SB_KEY")

start_llm()
bot  = Bot(TG_TOKEN, parse_mode="HTML")
dp   = Dispatcher(bot)
supabase: Client = create_client(SB_URL, SB_KEY)

# set Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹, Ñƒ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚ ĞµÑ‰Ñ‘ Ğ½Ğµ Ğ³Ğ¾Ñ‚Ğ¾Ğ²
PENDING: Set[int] = set()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ commands â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@dp.message_handler(commands=["start"])
async def cmd_start(m: types.Message):
    await m.reply("ğŸ‘‹ <b>ĞŸÑ€Ğ¸Ğ²ĞµÑ‚!</b>\nĞĞ¿Ğ¸ÑˆĞ¸ ÑĞ²Ğ¾Ğ¹ Ğ±Ğ¸Ğ·Ğ½ĞµÑ â€” Ñ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ¶Ñƒ, ĞºĞ°Ğº Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ´ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½ÑƒÑ Ğ¸Ğ´ĞµÑ.")

@dp.message_handler(commands=["help"])
async def cmd_help(m: types.Message):
    await m.reply("â„¹ï¸ ĞŸÑ€Ğ¾ÑÑ‚Ğ¾ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²ÑŒ ĞºÑ€Ğ°Ñ‚ĞºĞ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ±Ğ¸Ğ·Ğ½ĞµÑĞ°.\nĞĞµ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ» Ğ¸Ğ´ĞµÑ? Ğ¡Ğ´ĞµĞ»Ğ°Ğ¹ ÑÑ‚Ğ¾ Ğ½Ğ° ÑĞ°Ğ¹Ñ‚Ğµ Ğ²Ğ¾ Ğ²ĞºĞ»Ğ°Ğ´ĞºĞµ Â«Ğ˜Ğ´ĞµĞ¸Â».")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _build_prompt(user: dict, idea: dict, question: str) -> str:
    return PROMPT_TMPL.format(
        region     = user.get("region")           or "â€”",
        sector     = user.get("business_sector")  or "â€”",
        exp        = user.get("experience_level") or "â€”",
        goal       = user.get("transition_goal")  or "â€”",
        idea_title = idea["title"],
        idea_descr = idea["description"],
        question   = question.strip()
    )

@dp.message_handler(lambda ms: ms.text and not ms.text.startswith("/"))
async def biz_question(ms: types.Message):
    # ĞµÑĞ»Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚ ĞµÑ‰Ñ‘ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ â€” ÑĞ¾Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼ Ğ¸ Ğ¸Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€ÑƒĞµĞ¼
    if ms.from_user.id in PENDING:
        await ms.reply("â³ ĞÑ‚Ğ²ĞµÑ‚ ĞµÑ‰Ñ‘ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ, Ğ¿Ğ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ° Ğ´Ğ¾Ğ¶Ğ´Ğ¸Ñ‚ĞµÑÑŒ Ğ¸ Ğ½Ğµ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞ¹Ñ‚Ğµ Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ.")
        return

    tg_tag = f"@{ms.from_user.username}" if ms.from_user.username else None
    if not tg_tag:
        await ms.reply("âŒ Ğ”Ğ¾Ğ±Ğ°Ğ²ÑŒ username Ğ² Telegram Ğ¸ ÑƒĞºĞ°Ğ¶Ğ¸ ĞµĞ³Ğ¾ Ğ² Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ğµ Ğ½Ğ° ÑĞ°Ğ¹Ñ‚Ğµ."); return

    user = _sb_exec(supabase.table("users").select("*").eq("telegram", tg_tag).limit(1).maybe_single())
    if not user:
        await ms.reply("âŒ Telegram Ğ½Ğµ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·Ğ°Ğ½ Ğº Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ ÑĞ°Ğ¹Ñ‚Ğ°."); return

    missing = [k for k in ("region", "business_sector") if not user.get(k)]
    if missing:
        await ms.reply("â„¹ï¸ Ğ—Ğ°Ğ¿Ğ¾Ğ»Ğ½Ğ¸ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ÑŒ Ğ½Ğ° ÑĞ°Ğ¹Ñ‚Ğµ (Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½ Ğ¸ ÑÑ„ĞµÑ€Ñƒ) â€” ÑĞ¾Ğ²ĞµÑ‚Ñ‹ Ğ±ÑƒĞ´ÑƒÑ‚ Ñ‚Ğ¾Ñ‡Ğ½ĞµĞµ.")

    idea_link = _sb_exec(supabase.table("userideas").select("idea_id").eq("user_id", user["user_id"]).limit(1).maybe_single())
    if not idea_link:
        await ms.reply("â— Ğ¡Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ²Ñ‹Ğ±ĞµÑ€Ğ¸ Ğ¸Ğ´ĞµÑ Ğ½Ğ° ÑĞ°Ğ¹Ñ‚Ğµ Ğ²Ğ¾ Ğ²ĞºĞ»Ğ°Ğ´ĞºĞµ Â«Ğ˜Ğ´ĞµĞ¸Â»."); return

    idea = _sb_exec(supabase.table("trendingideas").select("title, description").eq("idea_id", idea_link["idea_id"]).single())

    session_id = get_or_create_session(supabase, user["user_id"], idea_link["idea_id"])
    prompt     = _build_prompt(user, idea, ms.text)

    log_chat(supabase, session_id, user["user_id"], "user", ms.text)

    # ÑƒĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ÑĞµĞ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ¸Ñ‚ÑÑ
    ack = await ms.reply("ğŸ’¬ Ğ¯ Ğ´ÑƒĞ¼Ğ°Ñ Ğ½Ğ°Ğ´ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ¼ â€” ÑÑ‚Ğ¾ Ğ·Ğ°Ğ¹Ğ¼Ñ‘Ñ‚ Ğ´Ğ¾ 2 Ğ¼Ğ¸Ğ½ÑƒÑ‚. ĞŸĞ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ°, Ğ½Ğµ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞ¹Ñ‚Ğµ Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ.")
    PENDING.add(ms.from_user.id)

    try:
        answer = send_to_llm(prompt)
    except Exception as e:
        await ack.edit_text(f"âŒ LLM Ğ½Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¸Ğ»: {e}")
        PENDING.discard(ms.from_user.id)
        return

    log_chat(supabase, session_id, user["user_id"], "assistant", answer)

    kb = InlineKeyboardMarkup(row_width=2).add(
        InlineKeyboardButton("ğŸ—‚ Ğ§ĞµĞº-Ğ»Ğ¸ÑÑ‚",    callback_data=f"todo:{session_id}"),
        InlineKeyboardButton("ğŸ”„ Ğ”Ñ€ÑƒĞ³Ğ°Ñ Ğ¸Ğ´ĞµÑ", callback_data="new_idea")
    )
    await ack.edit_text(answer, reply_markup=kb)

    PENDING.discard(ms.from_user.id)

    _sb_exec(supabase.table("userhistory").insert({
        "user_id":     user["user_id"],
        "action_type": "llm_recommendation",
        "action_time": datetime.now(timezone.utc).isoformat(),
        "details":     f"prompt_len={len(ms.text)}"
    }))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ callbacks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@dp.callback_query_handler(lambda c: c.data == "new_idea")
async def new_idea(cb: types.CallbackQuery):
    await cb.message.answer("ğŸ” ĞŸĞµÑ€ĞµĞ¹Ğ´Ğ¸ Ğ½Ğ° ÑĞ°Ğ¹Ñ‚ Ğ¸ Ğ²Ñ‹Ğ±ĞµÑ€Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¸Ğ´ĞµÑ â€” Ñ Ğ¿Ğ¾Ğ´ÑÑ‚Ñ€Ğ¾ÑÑÑŒ!")
    await cb.answer()

@dp.callback_query_handler(lambda c: c.data.startswith("todo:"))
async def todo_list(cb: types.CallbackQuery):
    session_id = cb.data.split(":", 1)[1]
    last_q = _sb_exec(supabase.table("chat_messages").select("content").eq("session_id", session_id).eq("role", "user").order("created_at", desc=True).limit(1).single())["content"]

    sess = _sb_exec(supabase.table("chat_sessions").select("user_id, idea_id").eq("session_id", session_id).single())
    user = _sb_exec(supabase.table("users").select("*").eq("user_id", sess["user_id"]).single())
    idea = _sb_exec(supabase.table("trendingideas").select("title, description").eq("idea_id", sess["idea_id"]).single())

    todo_prompt = _build_prompt(user, idea, last_q) + "\n\nĞ¡Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞ¹ Ñ‡ĞµĞº-Ğ»Ğ¸ÑÑ‚ Ğº Ğ¿Ğ»Ğ°Ğ½Ñƒ Ğ²Ñ‹ÑˆĞµ."
    answer      = send_to_llm(todo_prompt)

    log_chat(supabase, session_id, user["user_id"], "assistant", answer)
    await cb.message.answer(answer)
    await cb.answer()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ RUN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    executor.start_polling(dp, skip_updates=True)