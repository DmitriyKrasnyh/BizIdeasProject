# backend/gpt_bot.py
"""
Ğ¢ĞµĞ»ĞµĞ³Ñ€Ğ°Ğ¼-Ğ±Ğ¾Ñ‚ Ğ´Ğ»Ñ BizIdeas: Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ LLM (serve.py)
Ğ¸ Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ Ğ²ĞµÑÑŒ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³ Ğ² Supabase.

â— Ğ°Ğ²Ñ‚Ğ¾Ğ¿Ğ¾Ğ´Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ LLM, ĞµÑĞ»Ğ¸ Ğ¾Ğ½ ĞµÑ‰Ñ‘ Ğ½Ğµ Ğ·Ğ°Ğ¿ÑƒÑ‰ĞµĞ½;
â— Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ² chat_sessions / chat_messages;
â— Ğ´Ğ°Ñ‘Ñ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚, ÑƒĞ´Ğ¾Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ¼;
â— Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ĞºĞ½Ğ¾Ğ¿ĞºÑƒ Â«ğŸ—‚ Ğ§ĞµĞº-Ğ»Ğ¸ÑÑ‚Â».
"""

import os, sys, subprocess, time, textwrap, psutil, requests
from pathlib  import Path
from uuid     import uuid4
from datetime import datetime, timezone

from aiogram import Bot, Dispatcher, types
from aiogram.types import InlineKeyboardMarkup, InlineKeyboardButton
from aiogram.utils import executor
from dotenv  import load_dotenv
from supabase import create_client, Client, AuthError

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ENV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()

TG_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
SB_URL   = os.getenv("VITE_SUPABASE_URL")
SB_KEY   = os.getenv("VITE_SUPABASE_ANON_KEY")

MODEL_PATH = Path(
    os.getenv("LLM_MODEL_PATH",
              r"backend\llm\models\mistral-7b-instruct-v0.2.Q4_K_M.gguf")
).resolve()

LLM_HOST  = os.getenv("LLM_HOST", "127.0.0.1")
LLM_PORT  = int(os.getenv("LLM_PORT", 8000))
SERVE_PY  = Path(__file__).parent / "llm" / "serve.py"
HEALTH_URL= f"http://{LLM_HOST}:{LLM_PORT}/health"
LLM_URL   = f"http://{LLM_HOST}:{LLM_PORT}/v1/completions"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ PROMPT Ğ¨ĞĞ‘Ğ›ĞĞ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PROMPT_TMPL = """
Ğ¢Ñ‹ â€” Ğ¾Ğ¿Ñ‹Ñ‚Ğ½Ñ‹Ğ¹ Ğ±Ğ¸Ğ·Ğ½ĞµÑ-ĞºĞ¾Ğ½ÑÑƒĞ»ÑŒÑ‚Ğ°Ğ½Ñ‚.
ĞÑ‚Ğ²ĞµÑ‡Ğ°Ğ¹ Ğ»Ğ°ĞºĞ¾Ğ½Ğ¸Ñ‡Ğ½Ğ¾ Ğ¸ Ğ¿Ğ¾-Ğ´ĞµĞ»Ñƒ, Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼, Ñ†Ğ¸Ñ„Ñ€Ñ‹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ÑÑ.

â—¾ Ğ ĞµĞ³Ğ¸Ğ¾Ğ½ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ°: {region}
â—¾ ĞÑ‚Ñ€Ğ°ÑĞ»ÑŒ:        {sector}
â—¾ ĞĞ¿Ñ‹Ñ‚:           {exp}
â—¾ Ğ¦ĞµĞ»ÑŒ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ°:  {goal}

ğŸ’¡ Ğ’Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ
Â«{idea_title}Â» â€” {idea_descr}

â“ Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ
{question}

===== Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° (ÑĞ¾Ğ±Ğ»ÑĞ´Ğ°Ğ¹ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾) =====
1. ğŸ” ĞšĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¹ Â«Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾Ğ·Â» (1â€“2 Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ).
2. âœ… 3 Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ñ… ÑˆĞ°Ğ³Ğ° (âª… 40 ÑĞ»Ğ¾Ğ² ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹).
3. âš ï¸ 2 Ñ€Ğ¸ÑĞºĞ° + ĞºĞ°Ğº Ğ¸Ñ… ÑĞ½ÑÑ‚ÑŒ.
4. ğŸ“ˆ ĞŸĞ»Ğ°Ğ½ Ğ½Ğ° 3 Ğ¼ĞµÑ: Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ° â€œÑˆĞ°Ğ³ â†’ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°â€.
5. â­ Ğ’Ğ¾Ğ¿Ñ€Ğ¾Ñ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ñƒ Ğ¾Ğ´Ğ½Ğ¸Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ (CTA).
=========================================

### ĞÑ‚Ğ²ĞµÑ‚
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ HELPERS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _sb_exec(req):
    """Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ğº Supabase, ÑÑ€Ğ°Ğ·Ñƒ Ğ±Ñ€Ğ¾ÑĞ°ĞµĞ¼ Ğ¸ÑĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞµ."""
    try:
        res = req.execute()
    except AuthError as e:
        raise RuntimeError(f"Supabase AuthError: {e}") from None

    data  = res.get("data")  if isinstance(res, dict) else getattr(res, "data", None)
    error = res.get("error") if isinstance(res, dict) else getattr(res, "error", None)
    if error:
        raise RuntimeError(error["message"])
    return data


def llm_running() -> bool:
    for p in psutil.process_iter(["cmdline"]):
        if "serve.py" in " ".join(p.info["cmdline"] or []) and str(LLM_PORT) in p.info["cmdline"]:
            return True
    return False


def start_llm() -> None:
    """Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ backend/llm/serve.py ĞºĞ°Ğº detached-Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ (ĞµÑĞ»Ğ¸ Ğ¾Ğ½ Ğ½Ğµ Ğ¿Ğ¾Ğ´Ğ½ÑÑ‚)."""
    if llm_running():
        print("âœ…  LLM ÑƒĞ¶Ğµ Ğ·Ğ°Ğ¿ÑƒÑ‰ĞµĞ½"); return

    if not SERVE_PY.exists():
        sys.exit("â›”  backend/llm/serve.py Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½")

    cmd = [sys.executable, str(SERVE_PY),
           "--host", LLM_HOST, "--port", str(LLM_PORT),
           "--model", str(MODEL_PATH)]
    print("â–¶ï¸  Ğ¡Ñ‚Ğ°Ñ€Ñ‚ÑƒÑ LLM:", " ".join(cmd))
    subprocess.Popen(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

    for _ in range(30):
        try:
            if requests.get(HEALTH_URL, timeout=1).ok:
                print("âœ…  LLM Ğ¿Ğ¾Ğ´Ğ½ÑÑ‚"); return
        except requests.RequestException:
            time.sleep(1)
    sys.exit("â›”  LLM Ğ½Ğµ Ğ¿Ğ¾Ğ´Ğ½ÑĞ»ÑÑ Ğ·Ğ° 30 ÑĞµĞº")


def send_to_llm(prompt: str) -> str:
    r = requests.post(
        LLM_URL,
        json={"prompt": prompt, "max_tokens": 512, "temperature": 0.7},
        timeout=180
    )
    r.raise_for_status()
    return r.json()["choices"][0]["text"].strip()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ SUPABASE-Ğ£Ğ¢Ğ˜Ğ›Ğ« â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_or_create_session(sb: Client, user_id: int, idea_id: int) -> str:
    row = _sb_exec(
        sb.table("chat_sessions")
          .select("session_id")
          .eq("user_id", user_id)
          .eq("idea_id", idea_id)
          .limit(1)
          .maybe_single()
    )
    if row:
        return row["session_id"]

    sid = str(uuid4())
    _sb_exec(sb.table("chat_sessions").insert({
        "session_id": sid,
        "user_id":    user_id,
        "idea_id":    idea_id,
        "title":      "Telegram chat"
    }))
    return sid


def log_chat(sb: Client, session_id: str, user_id: int,
             role: str, text: str) -> None:
    _sb_exec(sb.table("chat_messages").insert({
        "message_id": str(uuid4()),
        "session_id": session_id,
        "user_id":    user_id,
        "role":       role,
        "content":    text,
        "created_at": datetime.now(timezone.utc).isoformat()
    }))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ INIT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if not all([TG_TOKEN, SB_URL, SB_KEY]):
    sys.exit("â›”  .env Ğ½ĞµĞ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ (TG_TOKEN / SB_URL / SB_KEY)")

start_llm()
bot        = Bot(TG_TOKEN, parse_mode="HTML")
dp         = Dispatcher(bot)
supabase: Client = create_client(SB_URL, SB_KEY)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ /start /help â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@dp.message_handler(commands=["start"])
async def cmd_start(m: types.Message):
    await m.reply(
        "ğŸ‘‹ <b>ĞŸÑ€Ğ¸Ğ²ĞµÑ‚!</b>\n"
        "ĞĞ¿Ğ¸ÑˆĞ¸ ÑĞ²Ğ¾Ğ¹ Ğ±Ğ¸Ğ·Ğ½ĞµÑ â€” Ñ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ¶Ñƒ, ĞºĞ°Ğº Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ´ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½ÑƒÑ Ğ¸Ğ´ĞµÑ."
    )

@dp.message_handler(commands=["help"])
async def cmd_help(m: types.Message):
    await m.reply(
        "â„¹ï¸ ĞŸÑ€Ğ¾ÑÑ‚Ğ¾ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²ÑŒ ĞºÑ€Ğ°Ñ‚ĞºĞ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ±Ğ¸Ğ·Ğ½ĞµÑĞ°.\n"
        "ĞĞµ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ» Ğ¸Ğ´ĞµÑ? Ğ¡Ğ´ĞµĞ»Ğ°Ğ¹ ÑÑ‚Ğ¾ Ğ½Ğ° ÑĞ°Ğ¹Ñ‚Ğµ Ğ²Ğ¾ Ğ²ĞºĞ»Ğ°Ğ´ĞºĞµ Â«Ğ˜Ğ´ĞµĞ¸Â»."
    )

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CORE HANDLER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _build_prompt(user: dict, idea: dict, question: str) -> str:
    return PROMPT_TMPL.format(
        region      = user.get("region")            or "â€”",
        sector      = user.get("business_sector")   or "â€”",
        exp         = user.get("experience_level")  or "â€”",
        goal        = user.get("transition_goal")   or "â€”",
        idea_title  = idea["title"],
        idea_descr  = idea["description"],
        question    = question.strip()
    )

@dp.message_handler(lambda ms: ms.text and not ms.text.startswith("/"))
async def biz_question(ms: types.Message):

    tg_tag = f"@{ms.from_user.username}" if ms.from_user.username else None
    if not tg_tag:
        await ms.reply("âŒ Ğ”Ğ¾Ğ±Ğ°Ğ²ÑŒ username Ğ² Telegram Ğ¸ ÑƒĞºĞ°Ğ¶Ğ¸ ĞµĞ³Ğ¾ Ğ² Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ğµ Ğ½Ğ° ÑĞ°Ğ¹Ñ‚Ğµ."); return

    user = _sb_exec(
        supabase.table("users").select("*")
                .eq("telegram", tg_tag)
                .limit(1).maybe_single()
    )
    if not user:
        await ms.reply("âŒ Telegram Ğ½Ğµ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·Ğ°Ğ½ Ğº Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ ÑĞ°Ğ¹Ñ‚Ğ°."); return

    missing = [k for k in ("region", "business_sector") if not user.get(k)]
    if missing:
        await ms.reply("â„¹ï¸ Ğ—Ğ°Ğ¿Ğ¾Ğ»Ğ½Ğ¸ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ÑŒ Ğ½Ğ° ÑĞ°Ğ¹Ñ‚Ğµ (Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½ Ğ¸ ÑÑ„ĞµÑ€Ñƒ) â€” ÑĞ¾Ğ²ĞµÑ‚Ñ‹ Ğ±ÑƒĞ´ÑƒÑ‚ Ñ‚Ğ¾Ñ‡Ğ½ĞµĞµ.")

    idea_link = _sb_exec(
        supabase.table("userideas").select("idea_id")
                .eq("user_id", user["user_id"])
                .limit(1).maybe_single()
    )
    if not idea_link:
        await ms.reply("â— Ğ¡Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ²Ñ‹Ğ±ĞµÑ€Ğ¸ Ğ¸Ğ´ĞµÑ Ğ½Ğ° ÑĞ°Ğ¹Ñ‚Ğµ Ğ²Ğ¾ Ğ²ĞºĞ»Ğ°Ğ´ĞºĞµ Â«Ğ˜Ğ´ĞµĞ¸Â»."); return

    idea = _sb_exec(
        supabase.table("trendingideas")
                .select("title, description")
                .eq("idea_id", idea_link["idea_id"])
                .single()
    )

    session_id = get_or_create_session(supabase, user["user_id"], idea_link["idea_id"])
    prompt     = _build_prompt(user, idea, ms.text)

    log_chat(supabase, session_id, user["user_id"], "user", ms.text)

    try:
        answer = send_to_llm(prompt)
    except Exception as e:
        await ms.reply(f"âŒ LLM Ğ½Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¸Ğ»: {e}"); return

    log_chat(supabase, session_id, user["user_id"], "assistant", answer)

    kb = InlineKeyboardMarkup(row_width=2).add(
        InlineKeyboardButton("ğŸ—‚ Ğ§ĞµĞº-Ğ»Ğ¸ÑÑ‚",      callback_data=f"todo:{session_id}"),
        InlineKeyboardButton("ğŸ”„ Ğ”Ñ€ÑƒĞ³Ğ°Ñ Ğ¸Ğ´ĞµÑ",   callback_data="new_idea")
    )
    await ms.reply(answer, reply_markup=kb)

    _sb_exec(supabase.table("userhistory").insert({
        "user_id":     user["user_id"],
        "action_type": "llm_recommendation",
        "action_time": datetime.now(timezone.utc).isoformat(),
        "details":     f"prompt_len={len(ms.text)}"
    }))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CALLBACKS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@dp.callback_query_handler(lambda c: c.data == "new_idea")
async def new_idea(cb: types.CallbackQuery):
    await cb.message.answer("ğŸ” ĞŸĞµÑ€ĞµĞ¹Ğ´Ğ¸ Ğ½Ğ° ÑĞ°Ğ¹Ñ‚ Ğ¸ Ğ²Ñ‹Ğ±ĞµÑ€Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¸Ğ´ĞµÑ â€” Ñ Ğ¿Ğ¾Ğ´ÑÑ‚Ñ€Ğ¾ÑÑÑŒ!")
    await cb.answer()

@dp.callback_query_handler(lambda c: c.data.startswith("todo:"))
async def todo_list(cb: types.CallbackQuery):
    session_id = cb.data.split(":", 1)[1]
    last_q = _sb_exec(
        supabase.table("chat_messages")
                .select("content")
                .eq("session_id", session_id)
                .eq("role", "user")
                .order("created_at", desc=True)
                .limit(1).single()
    )["content"]

    # Ğ´Ğ¾ÑÑ‚Ğ°Ñ‘Ğ¼ Ğ¸Ğ´ĞµÑ Ğ¸ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ÑŒ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ñ‚Ğ´Ğ°Ñ‚ÑŒ Ğ² prompt
    sess = _sb_exec(supabase.table("chat_sessions")
                    .select("user_id, idea_id")
                    .eq("session_id", session_id).single())
    user = _sb_exec(supabase.table("users")
                    .select("*").eq("user_id", sess["user_id"]).single())
    idea = _sb_exec(supabase.table("trendingideas")
                    .select("title, description")
                    .eq("idea_id", sess["idea_id"]).single())

    todo_prompt = _build_prompt(user, idea, last_q) + "\n\nĞ¡Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞ¹ Ñ‡ĞµĞº-Ğ»Ğ¸ÑÑ‚ Ğº Ğ¿Ğ»Ğ°Ğ½Ñƒ Ğ²Ñ‹ÑˆĞµ."
    answer      = send_to_llm(todo_prompt)

    log_chat(supabase, session_id, user["user_id"], "assistant", answer)
    await cb.message.answer(answer)
    await cb.answer()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ RUN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    executor.start_polling(dp, skip_updates=True)
