"""
FastAPI-–æ–±—ë—Ä—Ç–∫–∞ –≤–æ–∫—Ä—É–≥ llama-cpp-python
--------------------------------------
GET  /health         -> {"status":"ok"}
POST /v1/completions -> OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π –æ—Ç–≤–µ—Ç (text-completion)
POST /run            -> —É–¥–æ–±–Ω—ã–π ¬´–±—ã—Å—Ç—Ä—ã–π¬ª —ç–Ω–¥-–ø–æ–π–Ω—Ç –¥–ª—è —Å—Ç–∞—Ä–æ–≥–æ —Ñ—Ä–æ–Ω—Ç–∞
"""

import argparse, os, uuid, pathlib
from fastapi import FastAPI
from pydantic import BaseModel
from llama_cpp import Llama

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ CLI ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
ap = argparse.ArgumentParser()
ap.add_argument("--model", required=True, help="GGUF file")
ap.add_argument("--host", default="127.0.0.1")
ap.add_argument("--port", type=int, default=8000)
args = ap.parse_args()

CTX_LEN   = int(os.getenv("LLM_CTX",     4096))
N_THREADS = int(os.getenv("LLM_THREADS", os.cpu_count() or 4))

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Model load (–æ–¥–∏–Ω —Ä–∞–∑) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
if "llm_singleton" not in globals():          # –∑–∞—â–∏—Ç–∞ –æ—Ç –¥–≤–æ–π–Ω–æ–≥–æ –∏–º–ø–æ—Ä—Ç–∞
    print("‚è≥ –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å ‚Ä¶")
    globals()["llm_singleton"] = Llama(
        model_path=args.model,
        n_ctx     =CTX_LEN,
        n_threads =N_THREADS,
        n_gpu_layers=0,      # CPU-only
    )
    print(f"‚úÖ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ ({CTX_LEN} ctx)")

llm = globals()["llm_singleton"]

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ FastAPI ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
app = FastAPI(title="BizIdeas-LLM", docs_url="/docs")


class CompletionReq(BaseModel):
    prompt: str
    max_tokens: int = 512
    temperature: float = 0.7
    stream: bool = False   # –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º, –≤—Å–µ–≥–¥–∞ False
    # –ø–æ–ª—è OpenAI-spec, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞–º –Ω–µ –Ω—É–∂–Ω—ã, –Ω–æ –º–æ–≥—É—Ç –ø—Ä–∏–ª–µ—Ç–µ—Ç—å
    model: str | None = None
    top_p: float | None = None
    n: int | None = None
    stop: list[str] | None = None


@app.get("/health")
def health():
    return {"status": "ok"}


def _generate(prompt: str,
              max_tokens: int = 512,
              temperature: float = 0.7,
              stop: list[str] | None = None,
              **_) -> str:                       # ‚Üê –ø—Ä–∏–Ω–∏–º–∞–µ–º –≤—Å–µ –ª–∏—à–Ω–∏–µ kw-args
    """
    –û–¥–∏–Ω —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–∑–æ–≤ –º–æ–¥–µ–ª–∏: text-completion.
    """
    out = llm.create_completion(
        prompt       = prompt,
        max_tokens   = max_tokens,
        temperature  = temperature,
        stop         = stop or ["###"],   # –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π stop-—Ç–æ–∫–µ–Ω
    )
    return out["choices"][0]["text"].lstrip()


@app.post("/v1/completions")
def completions(req: CompletionReq):
    txt = _generate(**req.dict())
    return {
        "id":     f"cmpl-{uuid.uuid4()}",
        "object": "text_completion",
        "choices": [{"text": txt, "index": 0, "finish_reason": "stop"}],
        "model":  "local",
    }


# ------- ¬´–±—ã—Å—Ç—Ä—ã–π¬ª —ç–Ω–¥-–ø–æ–π–Ω—Ç –¥–ª—è —Å—Ç–∞—Ä–æ–≥–æ —Ñ—Ä–æ–Ω—Ç–∞ ----------------
class RunReq(BaseModel):
    question: str
    profile : dict | None = None
    locale  : str | None = "ru"

PROMPT_TMPL = """
–¢—ã ‚Äî –æ–ø—ã—Ç–Ω—ã–π –±–∏–∑–Ω–µ—Å-–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç.  
–û—Ç–≤–µ—á–∞–π –ª–∞–∫–æ–Ω–∏—á–Ω–æ, —Ç–æ–ª—å–∫–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º, –Ω–æ —Å —Ü–∏—Ñ—Ä–∞–º–∏ –∏ –ø—Ä–∏–º–µ—Ä–∞–º–∏.

**–ö–æ–Ω—Ç–µ–∫—Å—Ç –∫–ª–∏–µ–Ω—Ç–∞**
- –†–µ–≥–∏–æ–Ω: {region}
- –û—Ç—Ä–∞—Å–ª—å: {sector}
- –û–ø—ã—Ç: {exp}
- –¶–µ–ª—å –ø–µ—Ä–µ—Ö–æ–¥–∞: {goal}

**–í—ã–±—Ä–∞–Ω–Ω–∞—è –∏–¥–µ—è**  
¬´{idea_title}¬ª ‚Äî {idea_descr}

**–ó–∞–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç–∞**  
{question}

**–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ (_—Å—Ç—Ä–æ–≥–æ –ø—Ä–∏–¥–µ—Ä–∂–∏–≤–∞–π—Å—è_)**  
1. üîç –ö–æ—Ä–æ—Ç–∫–∏–π ¬´–¥–∏–∞–≥–Ω–æ–∑¬ª (1‚Äì2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –∑–∞—á–µ–º —ç—Ç–∞ –∏–¥–µ—è –ø–æ–ª–µ–∑–Ω–∞ –≤ –µ–≥–æ —Å–∏—Ç—É–∞—Ü–∏–∏).  
2. ‚úÖ –¢—Ä–∏ –±—ã—Å—Ç—Ä—ã—Ö —à–∞–≥–∞ (‚™Ö 40 —Å–ª–æ–≤ –∫–∞–∂–¥—ã–π).  
3. üí° –ß—Ç–æ –º–æ–∂–µ—Ç –ø–æ–π—Ç–∏ –Ω–µ —Ç–∞–∫ ‚Äî 2 —Ä–∏—Å–∫-—Ñ–∞–∫—Ç–æ—Ä–∞ –∏ —Å–ø–æ—Å–æ–± –∏—Ö —Å–Ω–∏–∑–∏—Ç—å.  
4. üìà –î–∞–ª—å–Ω–µ–π—à–∏–µ –¥–µ–π—Å—Ç–≤–∏—è –Ω–∞ 3-–º–µ—Å—è—Ü–∞ (—Ç–∞–±–ª–∏—Ü–∞: —à–∞–≥ ‚Üí –º–µ—Ç—Ä–∏–∫–∞ —É—Å–ø–µ—Ö–∞).  
5. ‚è≠ ¬´–•–æ—á–µ—à—å –ø–æ–¥—Ä–æ–±–Ω—ã–π –ø–ª–∞–Ω?¬ª ‚Äî –∑–∞–¥–∞–π –≤–æ–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç—É –æ–¥–Ω–∏–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º.

### –û—Ç–≤–µ—Ç
"""


@app.post("/run")
def run(req: RunReq):
    prompt = PROMPT_TMPL.format(**req.dict())
    return {"answer": _generate(prompt)}


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Entrypoint ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        app,                         # ‚Üê –ø–µ—Ä–µ–¥–∞—ë–º –æ–±—ä–µ–∫—Ç, —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ –≤—Ç–æ—Ä–æ–≥–æ –∏–º–ø–æ—Ä—Ç–∞
        host=args.host,
        port=args.port,
        log_level="info"
    )
